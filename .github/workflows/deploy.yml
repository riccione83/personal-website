name: Deploy Website to S3 with Optimizations

on:
  push:
    branches: ["main", "master"]
  pull_request:
    branches: ["main", "master"]
  workflow_dispatch:

env:
  CLOUDFRONT_ENABLED: ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID != '' }}
  CLOUDFRONT_NON_WWW_ENABLED: ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID_NON_WWW != '' }}

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: "18"
          cache: "yarn"

      - name: Install dependencies
        run: yarn install --frozen-lockfile

      - name: Build app with optimizations
        run: NODE_ENV=production yarn build
        env:
          # Ensure production environment for best optimization
          NODE_ENV: production

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'eu-west-1' }}

      - name: Set XML sitemap content type (direct upload)
        run: |
          # Upload sitemap.xml with proper content type
          aws s3 cp dist/sitemap.xml s3://${{ secrets.AWS_S3_BUCKET }}/sitemap.xml \
            --content-type "application/xml; charset=UTF-8" \
            --metadata-directive REPLACE \
            --cache-control "public, max-age=86400"
            
          # Upload compressed versions with proper content type and encoding
          if [ -f "dist/sitemap.xml.gz" ]; then
            aws s3 cp dist/sitemap.xml.gz s3://${{ secrets.AWS_S3_BUCKET }}/sitemap.xml.gz \
              --content-type "application/xml; charset=UTF-8" \
              --content-encoding "gzip" \
              --metadata-directive REPLACE \
              --cache-control "public, max-age=86400"
          fi

          if [ -f "dist/sitemap.xml.br" ]; then
            aws s3 cp dist/sitemap.xml.br s3://${{ secrets.AWS_S3_BUCKET }}/sitemap.xml.br \
              --content-type "application/xml; charset=UTF-8" \
              --content-encoding "br" \
              --metadata-directive REPLACE \
              --cache-control "public, max-age=86400"
          fi

          # Upload robots.txt with proper content type
          aws s3 cp dist/robots.txt s3://${{ secrets.AWS_S3_BUCKET }}/robots.txt \
            --content-type "text/plain; charset=UTF-8" \
            --metadata-directive REPLACE \
            --cache-control "public, max-age=86400"

      - name: Deploy regular files to S3
        run: |
          # Upload non-compressed assets first, excluding sitemap files which were already uploaded
          aws s3 sync dist/ s3://${{ secrets.AWS_S3_BUCKET }} \
            --delete \
            --exclude "*.gz" \
            --exclude "*.br" \
            --exclude "sitemap.xml" \
            --exclude "sitemap.xml.gz" \
            --exclude "sitemap.xml.br" \
            --exclude "robots.txt" \
            --cache-control "public, max-age=31536000, immutable" \
            --metadata-directive REPLACE

      - name: Set HTML specific content and cache settings
        run: |
          # Set HTML files with proper charset and shorter cache with iOS compatibility headers
          aws s3 cp s3://${{ secrets.AWS_S3_BUCKET }}/index.html s3://${{ secrets.AWS_S3_BUCKET }}/index.html \
            --content-type "text/html; charset=UTF-8" \
            --metadata-directive REPLACE \
            --metadata '{"Cache-Control":"public, max-age=0, must-revalidate", "Vary":"Accept-Encoding, User-Agent", "X-Content-Type-Options":"nosniff", "X-XSS-Protection":"1; mode=block", "Referrer-Policy":"strict-origin-when-cross-origin"}' \
            --cache-control "public, max-age=0, must-revalidate" \
            --acl public-read

      - name: Install jq for JSON processing
        run: |
          apt-get update && apt-get install -y jq || sudo apt-get update && sudo apt-get install -y jq

      - name: Configure S3 bucket for website hosting
        run: |
          # Enable website hosting on the S3 bucket
          aws s3 website s3://${{ secrets.AWS_S3_BUCKET }} --index-document index.html --error-document index.html

          # Set up website configuration with CORS and proper error handling
          cat > website-config.json << 'EOF'
          {
            "IndexDocument": {
              "Suffix": "index.html"
            },
            "ErrorDocument": {
              "Key": "index.html"
            },
            "RoutingRules": [
              {
                "Condition": {
                  "HttpErrorCodeReturnedEquals": "404"
                },
                "Redirect": {
                  "Protocol": "https",
                  "HostName": "${{ secrets.AWS_S3_BUCKET_DOMAIN || 'www.riccardorizzo.eu' }}",
                  "ReplaceKeyWith": "index.html"
                }
              },
              {
                "Condition": {
                  "HttpErrorCodeReturnedEquals": "403"
                },
                "Redirect": {
                  "Protocol": "https",
                  "HostName": "${{ secrets.AWS_S3_BUCKET_DOMAIN || 'www.riccardorizzo.eu' }}",
                  "ReplaceKeyWith": "index.html"
                }
              }
            ]
          }
          EOF

          # Apply the website configuration
          aws s3api put-bucket-website --bucket ${{ secrets.AWS_S3_BUCKET }} --website-configuration file://website-config.json || echo "Could not apply website configuration"

      - name: Upload compressed JS files
        run: |
          # Upload JS files with correct content-type and encoding
          find dist -name "*.js.gz" | while read file; do
            aws s3 cp "$file" s3://${{ secrets.AWS_S3_BUCKET }}/${file#dist/} \
              --content-type "application/javascript; charset=UTF-8" \
              --content-encoding "gzip" \
              --cache-control "public, max-age=31536000, immutable" \
              --metadata-directive REPLACE
          done

          # Upload Brotli compressed JS if available
          find dist -name "*.js.br" | while read file; do
            aws s3 cp "$file" s3://${{ secrets.AWS_S3_BUCKET }}/${file#dist/} \
              --content-type "application/javascript; charset=UTF-8" \
              --content-encoding "br" \
              --cache-control "public, max-age=31536000, immutable" \
              --metadata-directive REPLACE
          done

      - name: Upload compressed CSS files
        run: |
          # Upload CSS files with correct content-type and encoding
          find dist -name "*.css.gz" | while read file; do
            aws s3 cp "$file" s3://${{ secrets.AWS_S3_BUCKET }}/${file#dist/} \
              --content-type "text/css; charset=UTF-8" \
              --content-encoding "gzip" \
              --cache-control "public, max-age=31536000, immutable" \
              --metadata-directive REPLACE
          done

          # Upload Brotli compressed CSS if available
          find dist -name "*.css.br" | while read file; do
            aws s3 cp "$file" s3://${{ secrets.AWS_S3_BUCKET }}/${file#dist/} \
              --content-type "text/css; charset=UTF-8" \
              --content-encoding "br" \
              --cache-control "public, max-age=31536000, immutable" \
              --metadata-directive REPLACE
          done

      - name: Upload compressed HTML files
        run: |
          # Upload HTML files with correct content-type and encoding
          find dist -name "*.html.gz" | while read file; do
            aws s3 cp "$file" s3://${{ secrets.AWS_S3_BUCKET }}/${file#dist/} \
              --content-type "text/html; charset=UTF-8" \
              --content-encoding "gzip" \
              --cache-control "public, max-age=0, must-revalidate" \
              --metadata-directive REPLACE
          done

          # Upload Brotli compressed HTML if available
          find dist -name "*.html.br" | while read file; do
            aws s3 cp "$file" s3://${{ secrets.AWS_S3_BUCKET }}/${file#dist/} \
              --content-type "text/html; charset=UTF-8" \
              --content-encoding "br" \
              --cache-control "public, max-age=0, must-revalidate" \
              --metadata-directive REPLACE
          done

      - name: Set CORS configuration
        run: |
          aws s3api put-bucket-cors --bucket ${{ secrets.AWS_S3_BUCKET }} --cors-configuration '{
            "CORSRules": [
              {
                "AllowedOrigins": ["*"],
                "AllowedMethods": ["GET", "HEAD"],
                "AllowedHeaders": ["*"],
                "ExposeHeaders": ["ETag"],
                "MaxAgeSeconds": 86400
              }
            ]
          }'

      - name: Set Security Headers for CloudFront
        if: ${{ env.CLOUDFRONT_ENABLED == 'true' }}
        run: |
          # Create a security headers policy (only if using CloudFront)
          POLICY_ID=$(aws cloudfront create-response-headers-policy --cli-input-json '{
            "ResponseHeadersPolicyConfig": {
              "Name": "SecurityHeadersPolicy",
              "SecurityHeadersConfig": {
                "StrictTransportSecurity": {
                  "Override": true,
                  "TTL": 63072000
                },
                "ContentTypeOptions": {
                  "Override": true
                },
                "XSSProtection": {
                  "Override": true,
                  "Protection": true,
                  "ModeBlock": true
                },
                "ReferrerPolicy": {
                  "Override": true,
                  "ReferrerPolicy": "strict-origin-when-cross-origin"
                },
                "ContentSecurityPolicy": {
                  "Override": true,
                  "ContentSecurityPolicy": "default-src 'self'; img-src 'self' data: https://*; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; connect-src 'self'"
                },
                "FrameOptions": {
                  "Override": true,
                  "FrameOption": "DENY"
                }
              },
              "Comment": "Security headers for enhanced website security"
            }
          }' --query 'ResponseHeadersPolicy.Id' --output text 2>/dev/null) || echo "Could not create response headers policy (might already exist)"

          # If we got a policy ID, attach it to the CloudFront distribution
          if [ ! -z "$POLICY_ID" ] && [ ! -z "${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}" ]; then
            echo "Attaching security headers policy to CloudFront distribution"
            
            # Get the current distribution config
            aws cloudfront get-distribution-config --id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} --output json > cf-config.json
            
            # Extract the ETag
            ETAG=$(jq -r '.ETag' cf-config.json)
            
            # Update the DefaultCacheBehavior to use the new policy
            jq --arg policy_id "$POLICY_ID" '.DistributionConfig.DefaultCacheBehavior.ResponseHeadersPolicyId = $policy_id' cf-config.json > cf-config-updated.json
            
            # Remove the ETag from the updated config
            jq 'del(.ETag)' cf-config-updated.json > cf-config-final.json
            
            # Update the distribution with the new config
            aws cloudfront update-distribution --id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} --distribution-config file://cf-config-final.json --if-match "$ETAG" || echo "Could not update CloudFront distribution with security headers policy"
          fi

      - name: Invalidate CloudFront cache (www)
        # Only run this step if CLOUDFRONT_DISTRIBUTION_ID is available
        run: |
          if [ ! -z "${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}" ]; then
            aws cloudfront create-invalidation \
              --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} \
              --paths "/*"
            echo "CloudFront cache invalidated for www distribution."
          else
            echo "CloudFront distribution ID not provided, skipping www invalidation."
          fi

      - name: Invalidate CloudFront cache (non-www)
        # Only run this step if CLOUDFRONT_DISTRIBUTION_ID_NON_WWW is available
        run: |
          if [ ! -z "${{ secrets.CLOUDFRONT_DISTRIBUTION_ID_NON_WWW }}" ]; then
            aws cloudfront create-invalidation \
              --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID_NON_WWW }} \
              --paths "/*"
            echo "CloudFront cache invalidated for non-www distribution."
          else
            echo "Non-www CloudFront distribution ID not provided, skipping non-www invalidation."
          fi
